{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqh4o8QCA7dfbSi7g+8mHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PramodGuvvala07/Sitafal-Task/blob/main/Sitafal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpLg00B8sbI0",
        "outputId": "24b72eb3-4267-4e69-a68e-0983139b89ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: https://www.stanford.edu/ https://www.stanford.edu/ https://www.stanford.edu/ https://www.stanford.edu/ https://und.edu/\n",
            "Question: What is the focus of research at the University of Chicago?\n",
            "Answer:\n",
            "The Department of Physics and Astronomy manages and manages the University's \"Science, Engineering, and Math\" Department at the University of Chicago, focusing on the fundamental physics and engineering disciplines. The Department is run by the College of Medicine and Dentistry, and is one of the first departments to recognize the emerging technology economy of the 21st century. The College recently announced that it would establish the Center for the Study of Technology (CSP\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss  # No longer gives an error\n",
        "\n",
        "class DataIngestion:\n",
        "    def __init__(self, urls):\n",
        "        self.urls = urls\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.embeddings = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def crawl_and_scrape(self):\n",
        "        for url in self.urls:\n",
        "            result = requests.get(url)\n",
        "            soup = BeautifulSoup(result.text, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "            self.process_content(text, url)\n",
        "\n",
        "    def process_content(self, text, url):\n",
        "        chunks = self.segment_content(text)\n",
        "        for chunk in chunks:\n",
        "            embedding = self.model.encode(chunk)\n",
        "            self.embeddings.append(embedding)\n",
        "            self.metadata.append(url)\n",
        "\n",
        "    def segment_content(self, text):\n",
        "        return text.split('\\n\\n')  # Simple segmentation by paragraphs\n",
        "\n",
        "    def store_embeddings(self):\n",
        "        embedding_matrix = np.array(self.embeddings).astype('float32')\n",
        "        index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
        "        index.add(embedding_matrix)\n",
        "        faiss.write_index(index, 'embeddings.index')\n",
        "\n",
        "class QueryHandler:\n",
        "    def __init__(self, index, model):\n",
        "        self.index = index\n",
        "        self.model = model\n",
        "\n",
        "    def handle_query(self, query):\n",
        "        query_embedding = self.model.encode(query)\n",
        "        D, I = self.index.search(np.array([query_embedding]).astype('float32'), k=5)\n",
        "        return I\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "class ResultGenerator:\n",
        "    def __init__(self):\n",
        "        self.llm = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "    def generate_result(self, relevant_chunks, user_query):\n",
        "        context = \" \".join(relevant_chunks)\n",
        "        prompt = f\"Context: {context}\\nQuestion: {user_query}\\nAnswer:\"\n",
        "        result = self.llm(prompt, max_length=150)\n",
        "        return result[0]['generated_text']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\"\n",
        "    ]\n",
        "\n",
        "    ingestion = DataIngestion(urls)\n",
        "    ingestion.crawl_and_scrape()\n",
        "    ingestion.store_embeddings()\n",
        "\n",
        "    index = faiss.read_index('embeddings.index')\n",
        "    query_handler = QueryHandler(index, ingestion.model)\n",
        "    user_query = \"What is the focus of research at the University of Chicago?\"\n",
        "    relevant_indices = query_handler.handle_query(user_query)\n",
        "\n",
        "    result_generator = ResultGenerator()\n",
        "    relevant_chunks = [ingestion.metadata[i] for i in relevant_indices[0]]\n",
        "    result = result_generator.generate_result(relevant_chunks, user_query)\n",
        "\n",
        "    print(result)\n"
      ]
    }
  ]
}